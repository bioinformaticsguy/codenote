---
title: "Short Read"
---
# Short Read Pipeline Documentation
This page serves as a comprehensive guide to the Short Read Pipeline. It tracks the versions of the pipeline used throughout the analysis, along with the exact terminal commands executed to run each step of the pipeline. By documenting both the pipeline versions and the commands, this page ensures reproducibility and transparency in the analysis workflow. Each section includes:

- Pipeline Version: The version of the pipeline that was utilized for the analysis.

- Terminal Commands: A detailed log of all commands executed in the terminal to run the pipeline.

- Execution Details: Descriptions of key steps involved, including any specific parameters or configurations used.

# Setting up the Environment
Below are the one time commands that are needed to setup the env.
```bash
conda create -n nf
conda activate nf
conda install bioconda::nextflow
conda install conda-forge::singularity
cd work/
mkdir raredisease/
cd raredisease/
mkdir workdir outdir
```

# Verify the instalations

Check Nextflow verison 
```bash
(nf)$ nextflow -version

      N E X T F L O W
      version 24.10.6 build 5937
      created 23-04-2025 16:53 UTC (18:53 CEST)
      cite doi:10.1038/nbt.3820
      http://nextflow.io
```

Check Singularity verison 
```bash
(nf)$ singularity -version
singularity version 3.8.6
```

# Run Command
```bash
nextflow run nf-core/raredisease -revision dev -profile bih,test --outdir /data/cephfs-1/home/users/alhassa_m/work/raredisease/outdir/2025-02-05_16-59-21/ -work-dir /data/cephfs-1/home/users/alhassa_m/work/raredisease/workdir/
```

Command with custom config file:

```bash
 nextflow run nf-core/raredisease -revision dev -profile test -c custom.config --outdir /data/cephfs-1/home/users/alhassa_m/scratch/001/ -work-dir /data/cephfs-1/home/users/alhassa_m/scratch/workdir/
```

# Why we need dev or master


```bash
(nf) [alhassa_m@hpc-cpu-204 raredisease]$ nextflow run nf-core/raredisease -profile bih,test --outdir /data/cephfs-1/home/users/alhassa_m/work/raredisease/o
utdir/2025-02-05_16-59-21/ -work-dir /data/cephfs-1/home/users/alhassa_m/work/raredisease/workdir/

 N E X T F L O W   ~  version 24.10.6

Project `nf-core/raredisease` is currently stuck on revision: dev -- you need to explicitly specify a revision with the option `-r` in order to use it
```



# May 19th 2025 Monday
Tried to run the pipline again with the same configurations, it ran for the first time with errors and then I started to recieve errors like below:

```code
(nf) [alhassa_m@hpc-cpu-75 scratch]$  nextflow run nf-core/raredisease -revision dev -profile test -c custom.config --outdir /data/cephfs-1/home/users/alhassa_m/scratch/outdir/001/ -work-dir /data/cephfs-1/home/users/alhassa_m/scratch/workdir/
Nextflow 25.04.2 is available - Please consider updating your version to it

 N E X T F L O W   ~  version 24.10.6

Pulling nf-core/raredisease ...
Disk quota exceeded
(nf) [alhassa_m@hpc-cpu-75 scratch]$
```


Also noted that whenever I run the nextflow pipline it creates the .nextflow.log folder in the scratch folder and in the parent directory as well. 
Currently the problem is that the home directory gets filled up and then there is no space to download the images. 
Once I deleted the .singularity folder in the home directory.
Running the pipeline agian works but it then stops soon because of space error. 

Updated the custom Config File and apparently the file is being used as we can see changes on the terminal:

```code
Institutional config options
  config_profile_name            : Test profile
  config_profile_description     : Custom Config
  config_profile_contact         : Ali Hassan (Institut fuer Humangenetik)
  config_profile_url             : N/A
```

## Current Config File

```python
params {
    config_profile_description = 'Custom Config'
    config_profile_contact     = 'Ali Hassan (Institut fuer Humangenetik)'
    config_profile_url         = 'N/A'
}

params {
    max_memory = 760.GB
    max_cpus   = 48
    max_time   = 72.h
}

process {
    resourceLimits = [
        memory: 760.GB,
        cpus: 48,
        time: 72.h
    ]
    executor = 'slurm'
    scratch  = '$SCRATCHO'
}

singularity {
    enabled = true
}

```


## Next Steps
1. Try to change the project folder in the nextflow run command or in config (projectDir).

```bash
Core Nextflow options
  revision                       : dev
  runName                        : pedantic_bernard
  containerEngine                : singularity
  launchDir                      : /data/cephfs-1/scratch/groups/kircher/users/alhassa_m
  workDir                        : /data/cephfs-1/home/users/alhassa_m/scratch/workdir
  projectDir                     : /data/cephfs-1/home/users/alhassa_m/.nextflow/assets/nf-core/raredisease
  userName                       : alhassa_m
  profile                        : test
  configFiles                    : /data/cephfs-1/home/users/alhassa_m/.nextflow/assets/nf-core/raredisease/nextflow.config, /data/cephfs-1/scratch/groups/kircher/users/alhassa_m/custom.config
```

# May 20th 2025 Tuesday
We haven't changed the project folder rather we have created simlinks for .singularity folder.

```code
ln -s work/.singularity/ .singularity
```


This lets the folder in the home directory but all the real data that takes the space is now in the work directory which has a lot of quotoa.

Finally the pipline finished Running
```python 
-[nf-core/raredisease] Pipeline completed successfully-
Completed at: 20-May-2025 15:48:30
Duration    : 56m 2s
CPU hours   : 11.2
Succeeded   : 1'004
```

The config file is also changed and the current version of the config file looks like this:

```python
workDir = '/data/cephfs-1/work/groups/kircher/users/alhassa_m/run_nf/workdir/'

params {
    config_profile_description = 'Custom Config'
    config_profile_contact     = 'Ali Hassan (Institut fuer Humangenetik)'
    config_profile_url         = 'N/A'
}

params {
    max_memory = 760.GB
    max_cpus   = 48
    max_time   = 72.h
}

process {
    resourceLimits = [
        memory: 760.GB,
        cpus: 48,
        time: 72.h
    ]
    executor = 'slurm'
    scratch  = '/data/cephfs-1/scratch/groups/kircher/users/alhassa_m/'
}

singularity {
    enabled = true
    cacheDir = '/data/cephfs-1/work/groups/kircher/users/alhassa_m/run_nf/workdir/singularity/'
}
```

Current run command is like this: 

```code
nextflow run nf-core/raredisease -revision dev -profile test -c custom.config --outdir /data/cephfs-1/scratch/groups/kircher/users/alhassa_m/outdir/
```

# May 22nd 2025 Thursday
Created a one liner to check which samples are one file each forword and reverse read.

Here is the one liner code:

```code 
search_dir="/data/cephfs-2/unmirrored/groups/kircher/SexDiversity"; prefix="DNA_"; digit_list=({01..49}); for digit in "${digit_list[@]}"; do pattern="*${prefix}${digit}*"; label="${prefix}${digit}"; count=$(find "$search_dir" -name "$pattern" | wc -l); if [ "$count" -eq 4 ]; then echo "$label"; fi; done
```

## Step-by-step explanation of the script

| Step  | Code snippet                                                          | What it does                                                          |
|-------|----------------------------------------------                         |-----------------------------------------------------------            |
| 1     | `search_dir="/data/cephfs-2/unmirrored/groups/kircher/SexDiversity"`  | Defines the directory to search files in.                             |
| 2     | `prefix="DNA_"`                                                       | Sets the prefix part of your file name pattern.                       |
| 3     | `digit_list=({01..49})`                                               | Creates a list/array of zero-padded numbers from 01 to 49.            |
| 4     | `for digit in "${digit_list[@]}"; do`                                 | Starts a loop over each digit in the list.                            |
| 5     | `pattern="*${prefix}${digit}*"`                                       | Builds a file search pattern like `*DNA_01*`, `*DNA_02*`, etc.        |
| 6     | `label="${prefix}${digit}"`                                           | Creates the label to print if the condition matches (e.g., `DNA_01`). |
| 7     | `count=$(find "$search_dir" -name "$pattern" \| wc -l)`               | Counts how many files in `$search_dir` match the pattern.             |
| 8     | `if [ "$count" -eq 4 ]; then echo "$label"; fi`                       | If count equals 4, prints the label.                                  |
| 9     | `done`                                                                | Ends the loop.                                                        |


### Notes:

- The `${prefix}${digit}` construct dynamically builds file name patterns.
- The `find` command searches files recursively in the specified directory.
- `wc -l` counts the number of matching files.
- The `if` condition can be changed to other numeric comparisons like `-gt`, `-ge`, etc.

## Sample Details
After checking this we found that following 8 samples have just ```one``` file per forword/reverse read:

```python
DNA_01
DNA_10
DNA_11
DNA_13
DNA_15
DNA_17
DNA_38
DNA_46
```

After checking this we found that following 30 samples have ```three``` files per forword/reverse read:

```python
DNA_02
DNA_03
DNA_04
DNA_05
DNA_06
DNA_09
DNA_14
DNA_16
DNA_18
DNA_19
DNA_20
DNA_22
DNA_23
DNA_24
DNA_26
DNA_27
DNA_28
DNA_29
DNA_30
DNA_31
DNA_32
DNA_35
DNA_36
DNA_37
DNA_39
DNA_44
DNA_45
DNA_47
DNA_48
DNA_49
```

After checking this we found that following 10 samples have ```three``` files per forword/reverse read:

```python
DNA_07
DNA_08
DNA_21
DNA_25
DNA_33
DNA_34
DNA_40
DNA_41
DNA_42
DNA_43
```

# May 28th 2025 Wednesday

```code 
 nextflow run nf-core/raredisease -revision dev -profile test,singularity -params-file params.yaml -c custom.config    
```

So this time I ran the pipline with just 8GB memory and just 1 core with the command mentioned above. 

Though I have changed the work directory as well as the out directory as previously home work folder got full and my pipline was struck in the middle.


# May 30th 2025 Wednesday
Since the pipline failed, [See Error Here](files/error.log){target="_blank"}:






That I ran last time so I am running this again on the small sample sheet. The difference is that the data is very small and it is the test data from the rare disease pipline.

| sample          | lane | fastq_1                                                                                                                         | fastq_2                                                                                                                         | sex | phenotype | paternal_id | maternal_id | case_id  |
|-----------------|------|----------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|-----|-----------|-------------|-------------|----------|
| hugelymodelbat  | 1    | /data/cephfs-2/unmirrored/groups/kircher/users/alhassa_m/rare_data/1_171015_HHT5NDSXX_hugelymodelbat_XXXXXX_1.fastq.gz | /data/cephfs-2/unmirrored/groups/kircher/users/alhassa_m/rare_data/1_171015_HHT5NDSXX_hugelymodelbat_XXXXXX_2.fastq.gz | 1   | 2         | 0           | 0           | justhusky |


```code
sample,lane,fastq_1,fastq_2,sex,phenotype,paternal_id,maternal_id,case_id
hugelymodelbat,1,/data/cephfs-2/unmirrored/groups/kircher/users/alhassa_m/rare_data/1_171015_HHT5NDSXX_hugelymodelbat_XXXXXX_1.fastq.gz,/data/cephfs-2/unmirrored/groups/kircher/users/alhassa_m/rare_data/1_171015_HHT5NDSXX_hugelymodelbat_XXXXXX_2.fastq.gz,1,2,0,0,justhusky
```

## Two Reasons for the Error:
1. I was adding singulatity as a profile in the run command. 
2. Then I also noticed that there was genome GRCH38 mentioned in the prams file. 

## Solution
- so first I removed the run command. 
- then I also removed the genome from the prams file. 

## Update
So far I have been able to run the pipeline successfully with the data from the raredisease pipline. I just used 2 files which were given on the github page in the signleton sample sheet. 


```python 
-[nf-core/raredisease] Pipeline completed successfully-
Completed at: 30-May-2025 16:49:16
Duration    : 20m 5s
CPU hours   : 3.6
Succeeded   : 506
```

Next I am gonna run the pipline with my data using the updated prams file.

current params file:

```code
input: "/data/cephfs-1/work/groups/kircher/users/alhassa_m/run_nf/samplesheet_small_single.csv"
outdir: "/data/cephfs-1/scratch/groups/kircher/users/alhassa_m/data_out_nf/outdir/004_sfb_data_no_genome_prams"
```

run command is as follows:

```python
nextflow run nf-core/raredisease -revision dev -profile test -params-file params.yaml -c custom.config
```

# June 3rd 2025 Tuesday
I checked the running jobs and found out the following:

```python
JobID      CPUs   Nodes   Memory(GB) NodeList        RunTime    TimeLimit    JobName
16665798   1      1       8          hpc-cpu-82      23:16:19   7-00:00:00   bash
16682580   6      1       36         hpc-cpu-14      04:18:00   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_STRUCTURAL_VARIANTS_CALL_SV_TIDDIT_TIDDIT_SV_(DNA_02)
16682500   6      1       36         hpc-cpu-10      04:28:45   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_QC_BAM_QUALIMAP_BAMQC_(DNA_02)
```

Script used for this is as below:

```bash


# Print table header
printf "%-10s %-6s %-7s %-10s %-15s %-10s %-12s %-20s\n" \
    "JobID" "CPUs" "Nodes" "Memory(GB)" "NodeList" "RunTime" "TimeLimit" "JobName"

# Loop through all your jobs
for job in $(squeue --user $USER --noheader --format="%i"); do
    scontrol show job "$job" | awk -v jobid="$job" '
    BEGIN {
        name="NA"; cpus="NA"; nodes="NA"; mem="NA";
        nodelist="NA"; runtime="NA"; timelimit="NA";
    }
    /JobName=/ {
        for (i=1;i<=NF;i++) {
            if ($i ~ /^JobName=/) name=substr($i,9);
        }
    }
    /NumCPUs=/ {
        for (i=1;i<=NF;i++) {
            if ($i ~ /^NumCPUs=/) cpus=substr($i,9);
            if ($i ~ /^NumNodes=/) nodes=substr($i,10);
        }
    }
    /TRES=/ {
        match($0, /mem=([0-9]+)([MG]?)B?/, m);
        if (m[1] != "") {
            if (m[2] == "G") mem = m[1];
            else if (m[2] == "M") mem = sprintf("%.1f", m[1]/1024);
            else mem = "NA";
        }
    }
    /NodeList=/ {
        for (i=1;i<=NF;i++) {
            if ($i ~ /^NodeList=/) nodelist=substr($i,10);
        }
    }
    /RunTime=/ {
        for (i=1;i<=NF;i++) {
            if ($i ~ /^RunTime=/) runtime=substr($i,9);
            if ($i ~ /^TimeLimit=/) timelimit=substr($i,11);
        }
    }
    END {
        printf "%-10s %-6s %-7s %-10s %-15s %-10s %-12s %-20s\n", \
               jobid, cpus, nodes, mem, nodelist, runtime, timelimit, name;
    }'
done
```



# June 4rd 2025 Wednesday


Last tome pipeline was struck at:  ```149 more processes waiting for task...``` so then I ran it again. 

Ran the pipeline and nf core spawns a lot of jobs intitially.

```code
[alhassa_m@hpc-login-1 ~]$ date +"%Y-%m-%d %H:%M:%S"
2025-06-04 10:46:05
[alhassa_m@hpc-login-1 ~]$ ./work/sh_scripts/specs_usage.sh
JobID      CPUs   Nodes   Memory(GB) NodeList        RunTime    TimeLimit    JobName
16695642   2      1       16         hpc-cpu-60      00:02:58   7-00:00:00   bash
16695689   6      1       36         hpc-cpu-6       00:01:11   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_FASTQC_(DNA_02_LNUMBER1)
16695690   6      1       36         hpc-cpu-90      00:01:11   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_FASTQC_(DNA_02_LNUMBER2)
16695691   6      1       36         hpc-cpu-141     00:01:11   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_ALIGN_FASTP_(DNA_02_LNUMBER2)
16695693   6      1       36         hpc-cpu-152     00:01:11   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_ALIGN_FASTP_(DNA_02_LNUMBER1)
16695694   6      1       36         hpc-cpu-153     00:01:11   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_FASTQC_(DNA_02_LNUMBER3)
16695695   6      1       36         hpc-cpu-95      00:01:11   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_ALIGN_FASTP_(DNA_02_LNUMBER3)
16695701   1      1       6          hpc-cpu-153     00:01:11   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_PREPARE_REFERENCES_BWA_INDEX_GENOME_(reference.fasta)
```


# June 10th 2025 Tuesday
Today the pipeline finished in 25 minutes with the test data. 

command to see the rows of both files as in one line
```code
(head -n 1 samplesheet_single.csv && tail -n +2 samplesheet_single.csv && tail -n +2 sssheet.csv) | column -s, -t | less -S
```

checked the sample sheet for both samples and found out that those are same except for the change in the names of the files.

```code
sample          lane  fastq_1                                                                                                                                fastq_2                                                                                                                                sex  phenotype  paternal_id  maternal_id  case_id
dna_01          1     /data/cephfs-2/unmirrored/groups/kircher/SexDiversity/250130_LH00253_0200_A22JF7LLT4_A4842_FASTQ/A4842_DNA_01_S1_L001_R1_001.fastq.gz  /data/cephfs-2/unmirrored/groups/kircher/SexDiversity/250130_LH00253_0200_A22JF7LLT4_A4842_FASTQ/A4842_DNA_01_S1_L001_R2_001.fastq.gz  1    2          0            0            dnafirst
hugelymodelbat  1     /data/cephfs-2/unmirrored/groups/kircher/users/alhassa_m/rare_data/1_171015_HHT5NDSXX_hugelymodelbat_XXXXXX_1.fastq.gz                 /data/cephfs-2/unmirrored/groups/kircher/users/alhassa_m/rare_data/1_171015_HHT5NDSXX_hugelymodelbat_XXXXXX_2.fastq.gz                 1    2          0            0            justhusky
```

Param file is as below:

```code
(nf) [alhassa_m@hpc-cpu-1 run_nf]$ less params.yaml
input: "/data/cephfs-1/work/groups/kircher/users/alhassa_m/run_nf/samplesheet_single.csv"
outdir: "/data/cephfs-1/scratch/groups/kircher/users/alhassa_m/data_out_nf/outdir/008_one_bih_DNA01"
```

sample sheet used below:

```code
(nf) [alhassa_m@hpc-cpu-1 run_nf]$ less samplesheet_single.csv
sample,lane,fastq_1,fastq_2,sex,phenotype,paternal_id,maternal_id,case_id
dna_01,1,/data/cephfs-2/unmirrored/groups/kircher/SexDiversity/250130_LH00253_0200_A22JF7LLT4_A4842_FASTQ/A4842_DNA_01_S1_L001_R1_001.fastq.gz,/data/cephfs-2/unmirrored/groups/kircher/SexDiversity/250130_LH00253_0200_A22JF7LLT4_A4842_FASTQ/A4842_DNA_01_S1_L001_R2_001.fastq.gz,1,2,0,0,dnafirst
```

Date and Time
```code
(nf) [alhassa_m@hpc-cpu-1 run_nf]$ date
Tue Jun 10 21:23:18 CEST 2025
```

Command
```code 
nextflow run nf-core/raredisease -revision dev -profile test -params-file params.yaml -c custom.config
```


It spawned a lot of jobs initially

```code
[alhassa_m@hpc-login-1 ~]$ /data/cephfs-1/home/users/alhassa_m/work/sh_scripts/specs_usage.sh
JobID      CPUs   Nodes   Memory(GB) NodeList        RunTime    TimeLimit    JobName
16742670   1      1       16         hpc-cpu-1       00:10:55   7-00:00:00   bash
16742699   6      1       36                         00:00:00   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_PREPARE_REFERENCES_GATK_SD_MT_(reference_mt.fa)
16742682   6      1       36         hpc-cpu-113     00:00:03   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_ALIGN_FASTP_(dna_01_LNUMBER1)
16742683   6      1       36         hpc-cpu-3       00:00:03   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_FASTQC_(dna_01_LNUMBER1)
16742690   6      1       36         hpc-cpu-59      00:00:03   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_PREPARE_REFERENCES_GATK_SD_(reference.fasta)
16742701   1      1       6                          00:00:00   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_PREPARE_REFERENCES_SAMTOOLS_FAIDX_MT_(reference_mt.fa)
16742700   1      1       6                          00:00:00   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_PREPARE_REFERENCES_BWAMEM2_INDEX_MT_(reference_mt.fa)
16742698   1      1       6                          00:00:00   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_PREPARE_REFERENCES_TABIX_BGZIPINDEX_PADDED_BED_(target)
16742697   1      1       6                          00:00:00   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_SNV_CALL_SNV_DEEPVARIANT_ADD_VARCALLER_TO_BED_(deepvariant)
16742696   1      1       6                          00:00:00   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_SNV_POSTPROCESS_MT_CALLS_ADD_VARCALLER_TO_BED_(mutect2)
16742684   1      1       6          hpc-cpu-219     00:00:03   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CREATE_HGNCIDS_FILE_(standard)
16742685   1      1       6          hpc-cpu-219     00:00:03   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_PREPARE_REFERENCES_TABIX_PBT_(target)
16742688   1      1       6          hpc-cpu-13      00:00:03   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_PREPARE_REFERENCES_BWA_INDEX_GENOME_(reference.fasta)
16742689   1      1       6          hpc-cpu-13      00:00:03   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_PREPARE_REFERENCES_BWAMEM2_INDEX_GENOME_(reference.fasta)
```


# 12th June 2025

```code
rsync test.config bihlogin1:/data/cephfs-1/home/users/alhassa_m/work/run_nf/

nextflow run nf-core/raredisease -revision dev -profile test -params-file params.yaml -ansi-log false > nextflow_2025.06.12.out
```


# 24th June 2025

So there was some problem with running the pipeline and the was wrror porring up to the peddy as i accidently deleted the nextflow config file. 
I have created the git hub repo with the current release it is able to finish the pipeline with the following command the files can be found in the repo.

[rare-disease-pipeline v1.0.0-beta release](https://github.com/bioinformaticsguy/rare-disease-pipeline/releases/tag/v1.0.0-beta)

```code
nextflow run nf-core/raredisease -revision dev -profile test -c custom.config --outdir /data/cephfs1/scratch/groups/kircher/users/alhassa_m/outdir/ -resume        
```

## Command used for custom config

```code
 nextflow run nf-core/raredisease -revision dev  -c custom.config --outdir /data/cephfs-1/scratch/groups/kircher/users/alhassa_m/outdir/ -params-file params.yaml --skip_tools gens,germlinecnvcaller 

```

last command that I was trying:


```code
nextflow run nf-core/raredisease -revision dev  -c custom.config --outdir /data/cephfs-1/scratch/groups/kircher/users/alhassa_m/outdir/ -params-file params.yaml --skip_tools gens,germlinecnvcaller --skip_workflows mt_annotation,mt_subsample -resume 
```

So I ran it with 2.5.0 -revision and it got struck at 15 more steps and then I came back after lunch and restarted it with resule and it finished in 3 minutes.

## Anomly in the pipeline
I was running the nf-core/raredisease pipeline on a small test dataset with the latest release 2.5.0 (which had previously completed successfully). However, this time the run was taking much longer than expected. Even after a long wait, no new jobs were being spawned by Nextflow, and there was no visible progress. I became skeptical that the pipeline might be stuck.

I eventually terminated the run and restarted it using the -resume flag. Surprisingly, the pipeline then completed in just 3 minutes.

This is quite strange. Initially, I thought this behavior might be related to the BIH cluster environment (e.g. resource issues, scheduler problems), but since it also happens with very small test data, I’m not sure if this is a Nextflow issue or something cluster-specific.

For context, I experienced a similar situation earlier while running the pipeline on real data (~75GB input), where I assumed at the time that it was due to insufficient resources in the config file — but now I suspect there might be more to it.

```python
nextflow run nf-core/raredisease -revision 2.5.0 -profile test -c custom.config --outdir /data/cephfs-1/scratch/groups/kircher/users/alhassa_m/outdir/ -resume


-[nf-core/raredisease] Pipeline completed successfully-
Completed at: 24-Jun-2025 16:31:30
Duration    : 3m 21s
CPU hours   : 7.5 (99.7% cached)
Succeeded   : 7
Cached      : 995
```




## Currently missing files
when using the following command:


```
 nextflow run nf-core/raredisease -revision 2.5.0 -c custom.config -params-file params.yaml --outdir /data/cephfs-1/scratch/groups/kircher/users/alhassa_m/outdir/
```


### Need to generate correct files for grch38
```python
intervals_wgs
intervals_y
```

### These are present in the test profile but test profile is not for grch38
```python
params.svdb_query_dbs should be set.
params.vep_filters
params.variant_catalog not set.
params.vcfanno_resources not set.
params.vcfanno_toml not set.
params.vep_cache not set.
params.gnomad_af not set.
params.score_config_snv not set.
params.variant_consequences_snv not set.
params.score_config_sv not set.
params.variant_consequences_sv not set.
params.mobile_element_references not set.
params.mobile_element_svdb_annotations not set.
```

### Resolved by using skip_germlinecnvcaller = true in the config file

```python
params.ploidy_model not set.
params.gcnvcaller_model not set.
params.readcount_intervals not set.
```

I am also testing it just by using test profile and minimal options in the runn command
to use it in future to show that pipeline runs fine without errors at certain points

Here is the command and I will update if it successfuly finished or not tomorrow:

```python
nextflow run nf-core/raredisease -revision 2.5.0 -profile test -c minimal.config --outdir /data/cephfs-1/scratch/groups/kircher/users/alhassa_m/outdir/ 
```

So this pipeline got struck and was needed to be canceled and re-run with -resume tag and it finished in 10m.

# 25th June 2025
Max Suggested a lot of potential things that can help in introducing latency to the nextflow pipeline.

```python
//uncomment for SLURM cluster
//process {
//  withLabel: longtime {
//    executor='slurm'
//    //queue='long'
//    clusterOptions = '-t 3-00:00:0 --mem=6G'
//  }
//  withLabel: shorttime {
//    executor='slurm'
//    //queue='short'
//    clusterOptions = '-t 00-01:00:0 --mem=6G'
//  }
//  withLabel: highmem {
//    executor='slurm'
//    //queue='short'
//    clusterOptions = '-t 00-20:00:0 --mem=80G'
//  }
//}


https://github.com/shendurelab/MPRAflow/blob/f67cb0ba334d02b14645b697a03d89e915cd1366/association.nf#L269-L293
https://www.nextflow.io/docs/latest/executor.html#slurm
https://www.nextflow.io/docs/latest/reference/process.html#process-cpus


executor {
    name='slurm'
    queueSize=20
}
https://www.nextflow.io/docs/latest/reference/config.html#executor
executor.pollInterval
executor.queueSize
executor.retry.delay
executor.submitRateLimit
```

# 26th June 2025
Latest version of raredisease was released and Today I have noticed that even in the latest version when pipeline completed deepvarient tasks and jobs it got struck. 

```python
[alhassa_m@hpc-login-1 work]$ /data/cephfs-1/home/users/alhassa_m/work/sh_scripts/specs_usage.sh
JobID      CPUs   Nodes   Memory(GB) NodeList        RunTime    TimeLimit    JobName
16838856   1      1       64         hpc-cpu-80      2-16:49:52 7-00:00:00   bash
16924597   12     1       72         hpc-cpu-201     00:03:44   16:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_SNV_CALL_SNV_DEEPVARIANT_DEEPVARIANT_(earlycasualcaiman)
16924586   12     1       72         hpc-cpu-102     00:03:50   16:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_SNV_CALL_SNV_DEEPVARIANT_DEEPVARIANT_(hugelymodelbat)
```

So I stopped the job and resumed and figured out that after resuming it started almost 20 jobs:

```python
[alhassa_m@hpc-login-1 work]$ /data/cephfs-1/home/users/alhassa_m/work/sh_scripts/specs_usage.sh
JobID      CPUs   Nodes   Memory(GB) NodeList        RunTime    TimeLimit    JobName
16838856   1      1       64         hpc-cpu-80      2-17:03:39 7-00:00:00   bash
16925267   6      1       36                         00:00:00   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_REPEAT_EXPANSIONS_SPLIT_MULTIALLELICS_EXP_(hugelymodelbat)
16925266   6      1       36                         00:00:00   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_REPEAT_EXPANSIONS_SPLIT_MULTIALLELICS_EXP_(earlycasualcaiman)
16925245   6      1       36         hpc-cpu-87      00:00:10   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_ALIGN_CONVERT_MT_BAM_TO_FASTQ_GATK4_REVERTSAM_MT_(slowlycivilbuck)
16925246   6      1       36         hpc-cpu-207     00:00:10   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_ALIGN_CONVERT_MT_BAM_TO_FASTQ_GATK4_REVERTSAM_MT_(earlycasualcaiman)
16925235   6      1       36         hpc-cpu-87      00:00:14   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_ALIGN_CONVERT_MT_BAM_TO_FASTQ_GATK4_REVERTSAM_MT_(hugelymodelbat)
16925204   6      1       36         hpc-cpu-12      00:00:32   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_STRUCTURAL_VARIANTS_CALL_SV_MANTA_MANTA_(justhusky)
16925205   12     1       72         hpc-cpu-143     00:00:32   16:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_SNV_CALL_SNV_DEEPVARIANT_DEEPVARIANT_(earlycasualcaiman)
16925175   12     1       72         hpc-cpu-142     00:00:34   16:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_SNV_CALL_SNV_DEEPVARIANT_DEEPVARIANT_(hugelymodelbat)
16925192   12     1       72         hpc-cpu-143     00:00:34   16:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_SNV_CALL_SNV_DEEPVARIANT_DEEPVARIANT_(slowlycivilbuck)
16925268   1      1       6          hpc-cpu-80      00:00:00   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_STRUCTURAL_VARIANTS_CALL_SV_TIDDIT_SVDB_MERGE_TIDDIT_(justhusky)
16925256   1      1       6          hpc-cpu-82      00:00:04   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_STRUCTURAL_VARIANTS_CALL_SV_CNVNATOR_CNVNATOR_STAT_(hugelymodelbat)
16925257   1      1       6          hpc-cpu-80      00:00:04   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_STRUCTURAL_VARIANTS_CALL_SV_CNVNATOR_CNVNATOR_STAT_(slowlycivilbuck)
16925238   1      1       6          hpc-cpu-199     00:00:14   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_STRUCTURAL_VARIANTS_CALL_SV_CNVNATOR_CNVNATOR_HIST_(earlycasualcaiman)
16925201   1      1       6          hpc-cpu-212     00:00:32   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_QC_BAM_PICARD_COLLECTWGSMETRICS_Y_(earlycasualcaiman)
16925202   1      1       6          hpc-cpu-212     00:00:32   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_QC_BAM_PICARD_COLLECTWGSMETRICS_WG_(earlycasualcaiman)
16925203   1      1       6          hpc-cpu-212     00:00:32   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_QC_BAM_PICARD_COLLECTHSMETRICS_(earlycasualcaiman)
16925176   1      1       6          hpc-cpu-62      00:00:34   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_QC_BAM_PICARD_COLLECTWGSMETRICS_WG_(hugelymodelbat)
16925177   1      1       6          hpc-cpu-201     00:00:34   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_QC_BAM_PICARD_COLLECTWGSMETRICS_Y_(hugelymodelbat)
16925178   1      1       6          hpc-cpu-202     00:00:34   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_QC_BAM_PICARD_COLLECTMULTIPLEMETRICS_(hugelymodelbat)
16925188   1      1       6          hpc-cpu-86      00:00:34   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_QC_BAM_PICARD_COLLECTWGSMETRICS_WG_(slowlycivilbuck)
16925190   1      1       6          hpc-cpu-108     00:00:34   04:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_QC_BAM_PICARD_COLLECTWGSMETRICS_Y_(slowlycivilbuck)
```

Then it stopped again, and no new jobs were spawned in the BIH cluster at:

```python 
Plus 36 more processes waiting for tasks…
Staging foreign file: https://raw.githubusercontent.com/nf-core/test-datasets/raredisease/reference/vcfanno_cadd.tsv.gz
```

so I was able to complete the test run after resuming it for several times:

```python
nextflow run nf-core/raredisease -revision 2.6.0 -profile test -c minimal.config --outdir /data/cephfs-1/scratch/groups/kircher/users/alhassa_m/outdir/ -resume
```


In 2.6 version we have following files that are needed to be provided to run the pipeline:

```python
params.svdb_query_bedpedbs or params.svdb_query_dbs should be set.
params.vep_filters or params.vep_filters_scout_fmt should be set.
params.input not set.
params.variant_catalog not set.
params.vcfanno_resources not set.
params.vcfanno_toml not set.
params.vep_cache not set.
params.gnomad_af not set.
params.score_config_snv not set.
params.variant_consequences_snv not set.
params.score_config_sv not set.
params.variant_consequences_sv not set.
params.score_config_mt not set.
params.mobile_element_references not set.
params.mobile_element_svdb_annotations not set.
params.gens_gnomad_pos not set.
params.gens_interval_list not set.
params.gens_pon_female not set.
params.gens_pon_male not set.
params.ploidy_model not set.
params.gcnvcaller_model not set.
params.readcount_intervals not set.
```


Also these parameters are not good for this version:

```python
WARN: The following invalid input values have been detected:

* --pipelines_data_base_path: /data/cephfs-1/home/users/alhassa_m/work/run_nf/rare/data/grch38/
* --skip_germlinecnvcaller: true
* --skip_peddy: true
* --skip_haplogrep3: true
* --skip_vep_filter: true
* --max_memory: 760 GB
* --max_cpus: 48
* --max_time: 3d
```

So I skipped these tools and subworkflows and reduced the prerequsited to following 8 files only:

```python
    skip_tools                      = 'fastp,gens,haplogrep3,peddy,germlinecnvcaller,qualimap,eklipse,ngsbits'
    skip_subworkflows               = "mt_annotation,mt_subsample,me_calling,me_annotation,sv_annotation"
```


```python
params.vep_filters or params.vep_filters_scout_fmt should be set.
params.variant_catalog not set.
params.vcfanno_resources not set.
params.vcfanno_toml not set.
params.vep_cache not set.
params.gnomad_af not set.
params.score_config_snv not set.
params.variant_consequences_snv not set.
```


After skipping all the potential workflows, I am able to run the pipeline without need to provide any of the params.file. 

So now I tried to run this with the custom grch38 reference and there was problem with the name of the mitrochondria contg and that is needed to be adjusted in the config file.

- tried different versions of the genome, there was small problem with the MT chromosome name and it was needed to be adjusted.
- Also there was memory issue with the rule BWAMEM2_INDEX_GENOME as it will be problamatic even with 64 GB so now I am testing it with 256GB memory.


# 27th June 2025 

so I was finally able to finigh the pipeline by using my own config file and my own genome. I had to spend a lot of time to generate the intervals 
files but eventuallly it worked out. Now I just have to uncomment the different workflow steps one by one to see if everyting works. 

May be I will have to supply some more files.

# 30th June 2025
So I have figured that if the work directory is in the scratch then there is a lot of problem with running the pipeline and you will end up in stupid errors.
Well after fixing the workdirectory the piepline was able to finish successfully, now I am gonna try to run it on the big data to see what happens.

# 01st July 2025
So today I was running the pipeline with my real data and figured out that 8 hours is not enough for these two steps so that is why these two are needed to be run with 
more time and pipeline ran it with 16 hours of time now. 

```python
[c2/5f9ff8] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_SNV:POSTPROCESS_MT_CALLS:TABIX_ANNOTATE (dnafirst)                                   [100%] 1 of 1 ✔
[03/4d388e] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_MANTA:MANTA (dnafirst)                                   [ 50%] 1 of 2, failed: 1, retries: 1
[55/c918f5] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_TIDDIT:TIDDIT_SV (dna_01)                                [ 50%] 1 of 2, failed: 1, retries: 1
[b4/77797e] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_CNVNATOR:CNVNATOR_RD (dna_01)                            [100%] 1 of 1 ✔
[cb/3433fe] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_CNVNATOR:CNVNATOR_HIST (dna_01)                          [100%] 1 of 1 ✔
[77/c0b41b] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_CNVNATOR:CNVNATOR_STAT (dna_01)                          [100%] 1 of 1 ✔
[03/168c48] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_CNVNATOR:CNVNATOR_PARTITION (dna_01)                     [100%] 1 of 1 ✔
[6c/28733f] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_CNVNATOR:CNVNATOR_CALL (dna_01)                          [100%] 1 of 1 ✔
[58/d014e1] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_CNVNATOR:CNVNATOR_CONVERT2VCF (dna_01)                   [100%] 1 of 1 ✔
[53/e1c580] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_CNVNATOR:INDEX_CNVNATOR (dna_01)                         [100%] 1 of 1 ✔
[a4/c775aa] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_CNVNATOR:BCFTOOLS_VIEW_CNVNATOR (dna_01)                 [100%] 1 of 1 ✔
[18/b8cda5] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_CNVNATOR:SVDB_MERGE_CNVNATOR (dnafirst)                  [100%] 1 of 1 ✔
[b4/80d5a1] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_MT:MT_DELETION (dna_01)                                  [100%] 1 of 1 ✔
Plus 20 more processes waiting for tasks…
[17/d0daa7] NOTE: Process `NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_MANTA:MANTA (dnafirst)` terminated with an error exit status (140) -- Execution is retried (1)
[52/f4641b] NOTE: Process `NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_TIDDIT:TIDDIT_SV (dna_01)` terminated with an error exit status (140) -- Execution is retried (1)
```

## Proof
```python
[alhassa_m@hpc-login-1 ~]$ /data/cephfs-1/home/users/alhassa_m/work/sh_scripts/specs_usage.sh
JobID      CPUs   Nodes   Memory(GB) NodeList        RunTime    TimeLimit    JobName
16952976   1      1       56         hpc-cpu-78      2-05:06:20 7-00:00:00   bash
16982476   6      1       36         hpc-cpu-4       07:47:52   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_STRUCTURAL_VARIANTS_CALL_SV_TIDDIT_TIDDIT_SV_(dna_01)
16982480   12     1       72         hpc-cpu-95      07:47:52   16:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_SNV_CALL_SNV_DEEPVARIANT_DEEPVARIANT_(dna_01)
16982481   6      1       36         hpc-cpu-96      07:47:52   08:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_STRUCTURAL_VARIANTS_CALL_SV_MANTA_MANTA_(dnafirst)
16938187   1      1       16         hpc-cpu-60      4-04:21:19 7-00:00:00   bash
```

Now the new srun requests are as follow
```python
[alhassa_m@hpc-login-1 ~]$ /data/cephfs-1/home/users/alhassa_m/work/sh_scripts/specs_usage.sh
JobID      CPUs   Nodes   Memory(GB) NodeList        RunTime    TimeLimit    JobName
16952976   1      1       56         hpc-cpu-78      2-05:25:12 7-00:00:00   bash
16989740   12     1       72         hpc-cpu-59      00:07:19   16:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_STRUCTURAL_VARIANTS_CALL_SV_MANTA_MANTA_(dnafirst)
16989741   12     1       72         hpc-cpu-108     00:07:19   16:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_STRUCTURAL_VARIANTS_CALL_SV_TIDDIT_TIDDIT_SV_(dna_01)
16982480   12     1       72         hpc-cpu-95      08:06:44   16:00:00     nf-NFCORE_RAREDISEASE_RAREDISEASE_CALL_SNV_CALL_SNV_DEEPVARIANT_DEEPVARIANT_(dna_01)
16938187   1      1       16         hpc-cpu-60      4-04:40:11 7-00:00:00   bash
```

# 02nd July 2025 Wednesday

When I came to office finally the pipeline finished with one real sample saying... 

```python
-[nf-core/raredisease] Pipeline completed successfully-
Completed at: 02-Jul-2025 01:28:49
Duration    : 1d 8h 29m 7s
CPU hours   : 597.5 (16% failed)
Succeeded   : 83
Failed      : 2
```

The two failed tasks are the tasks that failed beause of less time these tasks were:

```python
[03/4d388e] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_MANTA:MANTA (dnafirst)                                   [100%] 2 of 2, failed: 1, retries: 1 ✔
[55/c918f5] process > NFCORE_RAREDISEASE:RAREDISEASE:CALL_STRUCTURAL_VARIANTS:CALL_SV_TIDDIT:TIDDIT_SV (dna_01)                                [100%] 2 of 2, failed: 1, retries: 1 ✔
```

So it is better to assign them 16 hours of time. 

# 03 July 2025 
So Last night I created a quick config as well as params file to use the small reference for debuggging purpose. I noticed that it ran fine in 15  minutes and then all of a sudden
I started reciving error messages as below and when I exited from that node I got this edit with eexit code 1. Normal exit from the node is also shown below:

```python
Command exit status:
  127

Command output:
  (empty)

Command error:
  .command.sh: line 4: bwa-mem2: command not found

Work dir:
  /data/cephfs-1/work/groups/kircher/users/alhassa_m/run_nf/rare/workdir/6c/9f7b3519ec4f08ad3023a6e558dae2

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`

 -- Check '.nextflow.log' file for details
ERROR ~ class java.util.ArrayList cannot be cast to class java.util.Map$Entry (java.util.ArrayList and java.util.Map$Entry are in module java.base of loader 'bootstrap')

 -- Check script '/data/cephfs-1/home/users/alhassa_m/.nextflow/assets/nf-core/raredisease/./workflows/../subworkflows/local/./call_sv_manta/main.nf' at line: 34 or see '.nextflow.log' file for more details
ERROR ~ Pipeline failed. Please refer to troubleshooting docs: https://nf-co.re/docs/usage/troubleshooting

 -- Check '.nextflow.log' file for details
ERROR ~ Pipeline failed. Please refer to troubleshooting docs: https://nf-co.re/docs/usage/troubleshooting

 -- Check '.nextflow.log' file for details

(nf) [alhassa_m@hpc-cpu-98 rare]$ exit
exit
srun: error: hpc-cpu-98: task 0: Exited with exit code 1
[alhassa_m@hpc-login-1 ~]$ srun -p medium --time 7-00:00 --mem=56G --immediate=60 --pty bash -i
[alhassa_m@hpc-cpu-115 ~]$ exit
exit
[alhassa_m@hpc-login-1 ~]$ 
```

# 04 July 2025

- Tried to solve this by re-requestng the new node. (Failed)
- Then I deleted the workdir (failed)
- Then I deleted the singularity cache. (failed)
- Then I gave it profile singularity it is apparently working, lets hope it finishes running.

but I got the following warning that is needed to be fixed. 

```python
WARN: Singularity cache directory has not been defined -- Remote image will be stored in the path: /data/cephfs-1/work/groups/kircher/users/alhassa_m/run_nf/rare/workdir/singularity -- Use the environment variable NXF_SINGULARITY_CACHEDIR to specify a different location
```

Finally the pipeline finished with the following command:

```python
nextflow run nf-core/raredisease -revision 2.6.0 -c quick.config -params-file quick.yaml -profile singularity    
```



Following are the parameters needed for not skipping any workflows.
```python
 17     vep_cache                      = params.pipelines_testdata_base_path + 'vep_cache_and_plugins.tar.gz'
 18     vcfanno_toml                   = params.pipelines_testdata_base_path + 'vcfanno_config.toml'
 19     vcfanno_resources              = params.pipelines_testdata_base_path + 'vcfanno_resources.txt'
 20     gnomad_af                      = params.pipelines_testdata_base_path + 'gnomad_reformated.tab.gz'
 21     score_config_snv               = params.pipelines_testdata_base_path + 'rank_model_snv.ini'
 22     variant_consequences_snv       = params.pipelines_testdata_base_path + 'variant_consequences_v2.txt'
 23     svdb_query_dbs                 = params.pipelines_testdata_base_path + 'svdb_querydb_files.csv'
 24     score_config_sv                = params.pipelines_testdata_base_path + 'rank_model_sv.ini'
 25     variant_consequences_sv        = params.pipelines_testdata_base_path + 'variant_consequences_v2.txt'
 26     vep_filters                    = params.pipelines_testdata_base_path + 'hgnc.txt'
 27     score_config_mt                = params.pipelines_testdata_base_path + 'rank_model_snv.ini'
 28     mobile_element_svdb_annotations = params.pipelines_testdata_base_path+ 'svdb_querydb_files.csv'
 29     mobile_element_references       = params.pipelines_testdata_base_path+ 'mobile_element_references.tsv'
```


```python
# Needed by the tool germlinecnvcaller
params.ploidy_model not set.
params.gcnvcaller_model not set.
params.readcount_intervals not set.
```

```python
# Tool removed from skip list

```


```python
# Work Fine without any extra parameters
haplogrep3
qualimap

```


## Tools with Errors
### smncopynumbercaller
There was error with chr number.


### NGS-Bits
```python
Plus 73 more processes waiting for tasks…
Execution cancelled -- Finishing pending tasks before exit
-[nf-core/raredisease] Pipeline completed with errors-
ERROR ~ Error executing process > 'NFCORE_RAREDISEASE:RAREDISEASE:QC_BAM:NGSBITS_SAMPLEGENDER (hugelymodelbat)'

Caused by:
  Process `NFCORE_RAREDISEASE:RAREDISEASE:QC_BAM:NGSBITS_SAMPLEGENDER (hugelymodelbat)` terminated with an error exit status (1)


Command executed:

  SampleGender \
      -in hugelymodelbat_sorted_md.bam \
      -method xy \
      -out hugelymodelbat_ngsbits_sex.tsv \
      -ref reference.fasta \


  cat <<-END_VERSIONS > versions.yml
  "NFCORE_RAREDISEASE:RAREDISEASE:QC_BAM:NGSBITS_SAMPLEGENDER":
      ngs-bits: $(echo $(SampleGender --version 2>&1) | sed 's/SampleGender //' )
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
  INFO:    Converting SIF file to temporary sandbox...
  SampleGender 2023_02
  Exception: Chromosome 'chrX' not known in BAM/CRAM file hugelymodelbat_sorted_md.bam
  Location : ../../src/cppNGS/BamReader.cpp:606
  INFO:    Cleaning up image...

Work dir:
  /data/cephfs-1/work/groups/kircher/users/alhassa_m/run_nf/rare/workdir/8b/e936ad41edf167e3351710509c090e

Container:
  /data/cephfs-1/work/groups/kircher/users/alhassa_m/run_nf/rare/workdir/singularity/depot.galaxyproject.org-singularity-ngs-bits-2023_02--py311ha0b7adc_2.img

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line

 -- Check '.nextflow.log' file for details
ERROR ~ Pipeline failed. Please refer to troubleshooting docs: https://nf-co.re/docs/usage/troubleshooting

 -- Check '.nextflow.log' file for details
```


# 6th July 2025
## Generation of Files
### params.mobile_element_references not set.

```python
1️⃣ Download RepeatMasker annotations from UCSC Table Browser:

Go to:
UCSC Table Browser

Settings:

clade: Mammal

genome: Human

assembly: GRCh38/hg38

group: Repeats

track: RepeatMasker

table: rmsk

region: genome

output format: BED

Filter for repeat class you need:


# Download rmsk from UCSC Table Browser as rmsk.bed
# Example filtering and sorting
awk '$4 ~ /Alu/' rmsk.bed | sort -k1,1 -k2,2n > grch38_alu.bed
awk '$4 ~ /L1/' rmsk.bed | sort -k1,1 -k2,2n > grch38_l1.bed
awk '$4 ~ /SVA/' rmsk.bed | sort -k1,1 -k2,2n > grch38_sva.bed
awk '$4 ~ /ERV1|HERV/' rmsk.bed | sort -k1,1 -k2,2n > grch38_herv.bed


BASE_PATH=$(pwd)
echo -e "type\tpath" > mobile_element_references_grch38.tsv
echo -e "L1\t${BASE_PATH}/grch38_l1.bed" >> mobile_element_references_grch38.tsv
echo -e "SVA\t${BASE_PATH}/grch38_sva.bed" >> mobile_element_references_grch38.tsv
echo -e "ALU\t${BASE_PATH}/grch38_alu.bed" >> mobile_element_references_grch38.tsv
echo -e "HERV\t${BASE_PATH}/grch38_herv.bed" >> mobile_element_references_grch38.tsv

```


### params.mobile_element_svdb_annotations not set.
```python
wget https://gnomad-public-us-east-1.s3.amazonaws.com/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz
gunzip gnomad.v4.1.sv.sites.vcf.gz


# Use following script to download the file:
echo "filename,in_freq_info_key,in_allele_count_info_key,out_freq_info_key,out_allele_count_info_key,use_in_freq_filter" > svdb_querydb_files.csv
echo "$(pwd)/gnomad.v4.1.sv.sites.vcf,AF,AC,gnomad_svAF,gnomad_svAC,1" >> svdb_querydb_files.csv
```

###  variant_consequences_snv        
Just downloaded from the sample files
```python
= params.pipelines_testdata_base_path + '/reference/variant_consequences_v2.txt'
```

### params.vcfanno_resources not set.
```python
params.vcfanno_toml not set.
params.gnomad_af not set.
params.score_config_snv not set.
```

# 15th Jully 2025

```python
https://raw.githubusercontent.com/nf-core/test-datasets/raredisease/reference/rank_model_snv.ini
```
This file we are using as it is as it appears to be not related to GrCh38.


So I started with working on mt_annotation and then because of the CADD files I moved to SNV annotation and now I have some problem as below:


```python
[5e/db798b] NFCORE_RAREDISEASE:RAREDISEASE:ANNOTATE_MOBILE_ELEMENTS:PICARD_SORTVCF (dnaSfirst)                       [100%] 1 of 1, cached: 1 ✔
[0a/f2f4d9] NFCORE_RAREDISEASE:RAREDISEASE:GENERATE_CYTOSURE_FILES:TIDDIT_COV_VCF2CYTOSURE (dnaS_01)                 [100%] 1 of 1, cached: 1 ✔
[-        ] NFCORE_RAREDISEASE:RAREDISEASE:MULTIQC                                                                   [  0%] 0 of 1
Plus 53 more processes waiting for tasks…
Execution cancelled -- Finishing pending tasks before exit
-[nf-core/raredisease] Pipeline completed with errors-
ERROR ~ Error executing process > 'NFCORE_RAREDISEASE:RAREDISEASE:ANNOTATE_GENOME_SNVS:BCFTOOLS_VIEW (dnaSfirst)'

Caused by:
  Process `NFCORE_RAREDISEASE:RAREDISEASE:ANNOTATE_GENOME_SNVS:BCFTOOLS_VIEW (dnaSfirst)` terminated with an error exit status (255)


Command executed:

  bcftools view \
      --output dnaSfirst_rhocall_filter_0001-scattered.vcf.gz \
       \
       \
       \
      --output-type z --exclude "INFO/GNOMADAF > 0.70 | INFO/GNOMADAF_popmax > 0.70"  \
      --threads 6 \
      dnaSfirst_rhocall_vcfanno_0001-scattered.vcf.gz

  cat <<-END_VERSIONS > versions.yml
  "NFCORE_RAREDISEASE:RAREDISEASE:ANNOTATE_GENOME_SNVS:BCFTOOLS_VIEW":
      bcftools: $(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*$//')
  END_VERSIONS

Command exit status:
  255

Command output:
  (empty)

Command error:
  INFO:    Converting SIF file to temporary sandbox...
  Error: cannot use arithmetic operators to compare strings and numbers
  INFO:    Cleaning up image...

Work dir:
  /data/cephfs-1/work/groups/kircher/users/alhassa_m/run_nf/rare/workdir/67/5ea006f1f346f39f3df89b4ade84e3

Container:
  /data/cephfs-1/home/users/alhassa_m/scratch/data_out_nf/singlecash/depot.galaxyproject.org-singularity-bcftools-1.20--h8b25389_0.img

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`

 -- Check '.nextflow.log' file for details
ERROR ~ Pipeline failed. Please refer to troubleshooting docs: https://nf-co.re/docs/usage/troubleshooting

 -- Check '.nextflow.log' file for details

(nf) [alhassa_m@hpc-cpu-173 rare]$
```


I think I wil have to check the files that are avalible for test and to see if my files have the same format! 

also I have noticed that the folder directory is quite messed up because of several folders so I need to fix that too..


# 17th July 2024
- Check if vcfanno_resoursces file is being used.
- clean up the directory.
- Check check gnomad file for chromosome 11.
- check the file for chromosome M. 
