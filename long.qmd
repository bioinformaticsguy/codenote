---
title: "Long Read"
---

Hi this page is for Long Read Documentation.

# Connecting to Bioscientia SFTP Server

Change the directory to the ssh and then, check if the keys are there.

```bash
cd ~/.ssh$
ls
```

If you get the following result then try the sftp command.

```bash
sftp -i private-key.pem kdhumuksh1@sftp2.bioscientia.de
```

Enter the passphrase and the you can see the files in the server.

## Important commands
To work with local and remote directories in an SFTP session, here are the key commands:

- lls — Lists the contents of the local directory (your machine).

- ls — Lists the contents of the remote directory (on the SFTP server).

- lcd <path> — Changes the local directory.

- cd <path> — Changes the remote directory.


## Downloading All Files from a Remote Directory
Once you've navigated to the desired directories using cd (remote) and lcd (local), you can download all files from the current remote directory to your current local directory using:
``` bash
mget *
```

## Example Output
You should see output similar to this in the terminal:

```bash
sftp> mget *
Fetching /export/r84078_20241114_111631/S011_X49-24_WGS-Rv-0156-report.pdf to S011_X49-24_WGS-Rv-0156-report.pdf
S011_X49-24_WGS-Rv-0156-report.pdf                                                                                        100% 3917KB  14.5MB/s   00:00

Fetching /export/r84078_20241114_111631/S011_X49-24_WGS-Rv-0156.hifi_reads.bam to S011_X49-24_WGS-Rv-0156.hifi_reads.bam
S011_X49-24_WGS-Rv-0156.hifi_reads.bam                                                                                    100%   34GB  55.6MB/s   10:25

Fetching /export/r84078_20241114_111631/S011_X49-24_WGS-Rv-0156.hifi_reads.bam.pbi to S011_X49-24_WGS-Rv-0156.hifi_reads.bam.pbi
S011_X49-24_WGS-Rv-0156.hifi_reads.bam.pbi                                                                                100%   79MB  44.1MB/s   00:01

```

# Running 3rd Trio Again
Run's Date: 05/05/2025 Monday

## Configuration
```bash
{
  "humanwgs_family.family": {
    "family_id": "ThirdTrio_156",
    "samples": [
      {
        "sample_id": "S009_X47-24_WGS-Rv-0155",
        "hifi_reads": ["/data/humangen_kircherlab/hassan/bioscientia/r84149R1_20241107_141501/S009_X47-24_WGS-Rv-0155.hifi_reads.bam"],
        "affected": true,
        "father_id": "S011_X49-24_WGS-Rv-0156",
        "mother_id": "S010_X48-24_WGS-Rv-0155"
      },
      {
        "sample_id": "S010_X48-24_WGS-Rv-0155",
        "hifi_reads": ["/data/humangen_kircherlab/hassan/bioscientia/r84149R1_20241107_141501/S010_X48-24_WGS-Rv-0155.hifi_reads.bam"],
        "affected": false,
        "sex": "FEMALE"
      },
      {
        "sample_id": "S011_X49-24_WGS-Rv-0156",
        "hifi_reads": ["/data/humangen_kircherlab/hassan/bioscientia/r84078_20241114_111631/S011_X49-24_WGS-Rv-0156.hifi_reads.bam"],
        "affected": false,
        "sex": "MALE"
      }
    ]
  },
  "humanwgs_family.phenotypes": "NA",
  "humanwgs_family.ref_map_file": "backends/hpc/GRCh38.ref_map.v2p0p0.hpc.tsv",
  "humanwgs_family.tertiary_map_file": "backends/hpc/GRCh38.tertiary_map.v2p0p0.hpc.tsv",
  "humanwgs_family.backend": "HPC",
  "humanwgs_family.gpu": true,
  "humanwgs_family.preemptible": true
}

```

## Run command
```bash
miniwdl run workflows/family.wdl -i backends/hpc/family.hpc.inputs.json --verbose
```

## resoursces
Ram 256 
Cores 42

the command used to request the resoursces is as follows:
```bash
 srun --partition=shortterm --mem=256G -c 42 --pty bash
```

# June 9rd 2025 Monday
List of potential SNV files that we can take a look.

## SNVs
### Joint 
```code
/data/humangen_kircherlab/hassan/HiFi-human-WGS-WDL/000_Old_Outputs/20250506_121608_humanwgs_family_2ndTrio_CN/out/tertiary_small_variant_compound_het_tsv/SecondTrio.joint.GRCh38.small_variants.phased.norm.slivar.compound_hets.tsv
vscode-remote://ssh-remote%2Bheadnode01.omics.uni-luebeck.de/data/humangen_kircherlab/hassan/HiFi-human-WGS-WDL/000_Old_Outputs/20250506_121608_humanwgs_family_2ndTrio_CN/out/tertiary_small_variant_filtered_tsv/SecondTrio.joint.GRCh38.small_variants.phased.norm.slivar.tsv
```
## Looked at IGV
```code
/data/humangen_kircherlab/hassan/HiFi-human-WGS-WDL/000_Old_Outputs/20250506_121608_humanwgs_family_2ndTrio_CN/out/trgt_spanning_reads/0/S005_X32-24_WGS-Rv-0150.GRCh38.trgt.spanning.sorted.bam
```

## Files that can be used in the report
```code
/data/humangen_kircherlab/hassan/HiFi-human-WGS-WDL/000_Old_Outputs/20250506_121608_humanwgs_family_2ndTrio_CN/out/stats_file/SecondTrio.stats.txt

```

# 30th June 2025

Today I created the small snakemake workflow so that the files needed are automatically generated which are missing. 
Now the gene lists are automatically created. I need to update the report, just add some headings. so that that we know what is needed to 
be filled in the report. Tomorrow I will also add the quarto report thingy so that the final html file is also generated. There
was some problem with the environments so I will generate envs folder and add specific envs so that the scripts can also run without having 
any env except just a snakemake env.

# 14 July 2025
## Meet With Niki
1. First we check if folowing file is annotated, after converting it to tsv.
```python
/data/humangen_kircherlab/hassan/HiFi-human-WGS-WDL/000_Old_Outputs/20250505_090954_humanwgs_family_3rd_156/out/tertiary_small_variant_filtered_vcf/ThirdTrio_156.joint.GRCh38.small_variants.phased.norm.slivar.vcf.gz
```

2. If yes then Check how many coding varinets are there are coding. (I expect something like 26.000 Variants per individual (Unfiltered)).

3. Then we can apply filtered to allele frequencies less than 0.01.

4. Prediction if they are deleterious (Think which tool to use).


# 18th July 2025

Today we are running the long read pipeline with the latest release 3.0.1 which has sawfish integerated.

## Step 1 (Download Pipeline)
I cloned the new version of the pipeline to a new folder through the following command. This command was found from this [link](https://github.com/PacificBiosciences/HiFi-human-WGS-WDL/tree/v3.0.1). 

```bash
git clone \
  --depth 1 --branch v3.0.1 \
  --recursive \
  https://github.com/PacificBiosciences/HiFi-human-WGS-WDL.git
```

## Step 2 (Create conda env)
I created a new conda env with the following command and activated it.

```bash
conda create -n latest_wdl
conda activate latest_wdl
```

## Step 3 (Install MiniWDL and MiniWDL-slurm)
It is important to mention the conda channel otherwise conda will end up in infinite loop. Moreover you can check the instalation and get a cool message at the end of the installation.

```bash
conda install -c conda-forge miniwdl 
miniwdl run_self_test
```

```bash
{
  "dir": "/tmp/miniwdl_run_self_test_ybtoold8/20250718_152641_hello_caller",
  "outputs": {
    "hello_caller.message_files": [
      "/tmp/miniwdl_run_self_test_ybtoold8/20250718_152641_hello_caller/out/message_files/0/Alyssa P. Hacker.txt",
      "/tmp/miniwdl_run_self_test_ybtoold8/20250718_152641_hello_caller/out/message_files/1/Ben Bitdiddle.txt"
    ],
    "hello_caller.messages": [
      "Hello, Alyssa P. Hacker!",
      "Hello, Ben Bitdiddle!"
    ]
  }
}

```


For miniwdl-slurm you can use the following command to install it.

```bash
pip install miniwdl-slurm
```


## Step 4 (Put MiniWDL Config file)
This config [file](files/miniwdl.cfg){target="_blank"} with cortrection of ```container_backend = slurm_singularity``` to ```container_backend = singularity``` is needed to be placed in the fillowing path.


```bash
[file](files/minimal.cfg){target="_blank"}
```


## Step 5 (Download the resourse Bundle)

The resourse bundle can be downloaded form this [link](https://github.com/PacificBiosciences/HiFi-human-WGS-WDL/blob/v3.0.1/docs/backend-hpc.md#:~:text=Reference%20data%20is%20hosted%20on%20Zenodo%20at%2010.5281/zenodo.15750792.%20Download%20the%20reference%20data%20bundle%20and%20extract%20it%20to%20a%20location%20on%20your%20HPC%2C%20then%20update%20the%20input%20template%20file%20with%20the%20path%20to%20the%20reference%20data.).

Once download extract it using the following command.

```bash
tar -xvf hifi-wdl-resources-v3.0.0.tar
```

## Step 6 (Fill Out Family Inputs JSON file and Paths file)
The following file is needed to be updated like this [file](files/family.hpc.inputs.jason){target="_blank"}.

```bash
backends/hpc/family.hpc.inputs.json
```

Also the paths are needed to be updated in the following files.

```bash
backends/hpc/family.hpc.inputs.json
backends/hpc/GRCh38.tertiary_map.v3p0p0.hpc.tsv
```

Step 7 (Run the Pipeline)

Try to get  64 cpu cores and 256 GB of RAM in a screen session. The following command can be used to request the resources.

```bash
hassan@headnode01:/data/humangen_kircherlab/hassan/wdl_latest/HiFi-human-WGS-WDL$ srun --partition=debug  --mem=128G -c 48 --pty bash
srun: job 1487609 queued and waiting for resources
```

We tried this much and the cluster is moody at this point.

# 29th July 2025
I got the following resooursces.
```bash
srun --partition=shortterm --mem=256G -c 4 --pty bash
```

Then ran the pipeline in the following dir with following command.

```bash
/data/humangen_kircherlab/hassan/wdl_latest/HiFi-human-WGS-WDL
miniwdl run workflows/family.wdl -i backends/hpc/family.hpc.inputs.json --verbose
```

it is apparently running

```bash
2025-07-29 13:46:51.574 wdl.w:humanwgs_family.w:call-upstream-2-S011_X4924_WGSRv.w:call-pbmm2-0.t:call-split_input_bam.singularity WARNING: Could not find any nv files on this host!
2025-07-29 13:46:51.574 wdl.w:humanwgs_family.w:call-upstream-1-S010_X4824_WGSRv.w:call-pbmm2-0.t:call-split_input_bam.singularity WARNING: Could not find any nv files on this host!
2025-07-29 13:46:51.575 wdl.w:humanwgs_family.w:call-upstream-0-S009_X4724_WGSRv.w:call-pbmm2-0.t:call-split_input_bam.singularity WARNING: Could not find any nv files on this host!
    0:01:37 elapsed    ▬▬▬        tasks finished: 0, ready: 0, running: 3    reserved CPUs: 48
```